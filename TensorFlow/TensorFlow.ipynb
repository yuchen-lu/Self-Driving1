{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####--Tensor\n",
    "# in tf, data not stored as int, float or strings\n",
    "# as a tensor, hello_constant is a 0-d string tensor\n",
    "# B is a 1-dimensional int32 tensor\n",
    "#B = tf.constant([123,456,789]) \n",
    "\n",
    "\n",
    "#####--Session\n",
    "# an envir for running a graph\n",
    "# creates a session instance, sess, using tf.Session\n",
    "# sess.run() evaluate tensor and turn result\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called hello_constant\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-constant: tf.placeholder() and feed_dict\n",
    "\n",
    "#####--tf.placeholder()\n",
    "# returns a tensor that gets its alue from data passed to tf.session.run()\n",
    "# allow you to set input before session runs\n",
    "\n",
    "#####---Session's feed_dict\n",
    "# use feed_dict param in tf.session.run() to set placeholder tensor\n",
    "# x being set to \"Hello World\"\n",
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x:\"Hello World\"})\n",
    "\n",
    "    \n",
    "# set more than one tensor\n",
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y:123, z: 45.67}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUIZ\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    x = tf.placeholder(tf.int32)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # TODO: Feed the x tensor 123\n",
    "        output = sess.run(x, feed_dict={x:123})\n",
    "\n",
    "    return output\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF with MATH!\n",
    "\n",
    "\n",
    "x = tf.add(5,2)\n",
    "x = tf.subtract(10,4)\n",
    "y = tf.multiply(2,5)\n",
    "\n",
    "# # converting types\n",
    "# tf.subtract( tf.constant(2.0), tf.constant(1))  WRONG!!!\n",
    "\n",
    "tf.subtract( tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUIZ\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO: Convert the following to TensorFlow:\n",
    "x = 10\n",
    "y = 2\n",
    "z = x/y - 1\n",
    "\n",
    "# TODO: Print z from a session\n",
    "\n",
    "x = tf.constant(10)\n",
    "# error!! : x = tf.int32(10)\n",
    "\n",
    "y = tf.constant(2)\n",
    "afterdivide = tf.divide(x,y)\n",
    "#afterint = tf.cast(afterdivide, tf.int32)\n",
    "\n",
    "z = tf.subtract(afterdivide,1)\n",
    "\n",
    "\n",
    "#z = tf.sub(tf.div(x, y), m)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    output = sess.run(z)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###---Classification\n",
    "##---logistic classifier\n",
    "\n",
    "# WX+b = y   weights and bias to be trained\n",
    "#softax---turn scroes to probablities\n",
    "# for the rest, we actually use y=xW+b\n",
    "\n",
    "#not using tfplaceholder() and tf.constant() since cannot be modified\n",
    "\n",
    "# tf.Variable() creates a tesor with an intial value that can be modified\n",
    "\n",
    "#init = tf.global_variables_initializer()\n",
    "#with tf.Session() as sess:\n",
    " #   sess.run(init)\n",
    "\n",
    "#tf.truncated_normal() generate random numbers from a normal distribution.\n",
    "\n",
    "import tensorflow as tf\n",
    "def get_weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # TODO: Return weights\n",
    "    return tf.Variable(tf.truncated_normal((n_features,n_labels)))\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # TODO: Return biases\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "\n",
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    return tf.add(tf.matmul(input,w),b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from quiz import get_weights, get_biases, linear\n",
    "\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # TODO: Initialize session variables\n",
    "    \n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    # TODO: Compute and return softmax(x)\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "\n",
    "logits = [3.0, 1.0, 0.2]\n",
    "print(softmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # TODO: Calculate the softmax of the logits\n",
    "    softmax  =tf.nn.softmax(logits)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # TODO: Feed in the logit data\n",
    "        output = sess.run(softmax, feed_dict={logits:logit_data})\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# one-hot encoding\n",
    "# we have prob for correct class and incorrect\n",
    "# encoding: change them to 1 and 0\n",
    "# value 1 for correct value, 0 for others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18\n",
    "# cross entropy\n",
    "# when one-hot encoding inefficient when large data matrix\n",
    "# D(S,L)    S is distribution(prob) L is labels(1 0 0)\n",
    "\n",
    "#x-->using wx+b-->logits y(scores)-->using softmax-->probability-->using cross-entropy-->1-hot labels\n",
    "# the whole process: multinomial logistic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19\n",
    "# quiz \n",
    "#tf.reduce_sum() function takes an array of numbers and sums them together.\n",
    "#tf.log()\n",
    "\n",
    "# task : print the cross entropy using softmax_data and one_hot_data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# TODO: Print cross entropy from session\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ouput = sess.run(cross_entropy, feed_dict={softmax:softmax_data, one_hot:one_hot_data})\n",
    "    print(output)\n",
    "\n",
    "    ### note: this program is not runable\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20\n",
    "# D(S(wx+b),L); goal: find w,b\n",
    "#D is distance, should be low for correct class; high D for incorrect\n",
    "# training loss = avg cross-entropy = 1/N sumi(D(S(wxi+b),Li))\n",
    "# to solve: method 1. gradient descent\n",
    "\n",
    "\n",
    "# 21 \n",
    "# tools of computing derivative\n",
    "# Two questions before start: 1. how to fill ig pixels to classcifier\n",
    "\n",
    "# 22\n",
    "# show numerical error in adding small number to a big number\n",
    "a=10000000\n",
    "counter =1\n",
    "for i  in range(1000000):\n",
    "\n",
    "    a = a+1e-6\n",
    "print(a-10000000)\n",
    "\n",
    "\n",
    "# 23\n",
    "# ROT: mean xi=0; variance(xi)=variance(xj)\n",
    "# (r-128)/128 same as g b \n",
    "# weights initilization: large sigma-->large peaks; small sigma_-->distribution very uncertain, better for start\n",
    "# xi = pixel-128/128; wi is random weight with mean =0 variance small; then find average\n",
    "# then optimization: keep updating w and b\n",
    "\n",
    "\n",
    "#24\n",
    "# model will try to memo data, have to generalize by using test data\n",
    "# furthermore, use validation set after test for final test\n",
    "\n",
    "# 25, 26, 27, 28, \n",
    "# transition overfitting\n",
    "# valida and test set size\n",
    "# ROT: rule of 30 ;see how many samples changed; min 30\n",
    "# eg. 3000 example, 80% -> 81%; good \n",
    "# validation set size > 30,000 examples; changes>0.1% in accuracy\n",
    "# for small examples, could use cross-validation, but slow\n",
    "\n",
    "# 29\n",
    "# training logistic regression: scaling issue\n",
    "\n",
    "# 30\n",
    "# stochastic gradient descent(S.G.D.)\n",
    "# take random data, compute loss and derivative, take many more small steps\n",
    "# scales well with  both data and model\n",
    "# only one thats fast enough, but comes with issues\n",
    "\n",
    "#31 Helping SGD\n",
    "#  momentum M =0.9M+delta error\n",
    "# learning rate decay: \n",
    "# inputs: mean =0, equal small variance ; initial weights: random!!!! mean =0; equal small var too\n",
    "\n",
    "# 32\n",
    "# always try to lower learning rate!!\n",
    "# ADAGRAD: auto does momentun and learning for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  33 mini-batching: techniques of training on subsets of dataset instead of all the data at one time\n",
    "#  provides ability to trainn a model,even computer lacks memeory to store all dataset\n",
    "#  is coputationlly INefficient, can't calculate the loss simultaneously across all samples, its the price\n",
    "#     combine with SGD: randomly shuffle the data at the start of each epoch, then create mini-batches, for each mini,\n",
    "#     train the network weights with gradient descent\n",
    "\n",
    "# memory size : float32 32bits 4 bytes\n",
    "\n",
    "\n",
    "#*****tensorflow mini-batching\n",
    "# tf.placeholder() function to receive the varying batch sizes.  need to divide data into batches\n",
    "\n",
    "\n",
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "        \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(features)==len(labels)\n",
    "    # TODO: Implement batching\n",
    "    output_batches[]\n",
    "    sample_size = len(features)\n",
    "    for i in range ( 0, sample_size, batch_size):\n",
    "        end_i = i + batch_size\n",
    "        batch = [features[i:end_i], labels[i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches\n",
    "    pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "/datasets; Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e15fbad9470b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Import MNIST data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/datasets/ud730/mnist'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# The features are already scaled and the data is shuffled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\u001b[0m in \u001b[0;36mread_data_sets\u001b[0;34m(train_dir, fake_data, one_hot, dtype, reshape, validation_size, seed, source_url)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m   local_file = base.maybe_download(TRAIN_IMAGES, train_dir,\n\u001b[0;32m--> 240\u001b[0;31m                                    source_url + TRAIN_IMAGES)\n\u001b[0m\u001b[1;32m    241\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py\u001b[0m in \u001b[0;36mmaybe_download\u001b[0;34m(filename, work_directory, source_url)\u001b[0m\n\u001b[1;32m    203\u001b[0m   \"\"\"\n\u001b[1;32m    204\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m   \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mrecursive_create_dir\u001b[0;34m(dirname)\u001b[0m\n\u001b[1;32m    366\u001b[0m   \"\"\"\n\u001b[1;32m    367\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursivelyCreateDir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: /datasets; Permission denied"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# TODO: Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # TODO: Train optimizer on all batches\n",
    "    # for batch_features, batch_labels in ______\n",
    "    batch_output = batches(batchsize, features, labels)\n",
    "    \n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "def batches(batch_size, features, labels):\n",
    "    assert len(features)==len(labels)\n",
    "    output_batches = []\n",
    "    sample_size = len(features)\n",
    "    for i in range ( 0, sample_size, batch_size):\n",
    "        end_i = i + batch_size\n",
    "        batch = [features[i:end_i], labels[i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches\n",
    "    \n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement batching\n",
    "    output_batches = []\n",
    "    sample_size = len(features)\n",
    "    for i in range ( 0, sample_size, batch_size):\n",
    "        end_i = i + batch_size\n",
    "        batch = [features[i:end_i], labels[i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 34\n",
    "#An epoch is a single forward and backward pass of the whole dataset. \n",
    "# This is used to increase the accuracy of the model without requiring more data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def batches(batch_size, features, labels):\n",
    "    assert len(features)==len(labels)\n",
    "    output_batches = []\n",
    "    sample_size = len(features)\n",
    "    for i in range ( 0, sample_size, batch_size):\n",
    "        end_i = i + batch_size\n",
    "        batch = [features[i:end_i], labels[i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches\n",
    "    \n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement batching\n",
    "    output_batches = []\n",
    "    sample_size = len(features)\n",
    "    for i in range ( 0, sample_size, batch_size):\n",
    "        end_i = i + batch_size\n",
    "        batch = [features[i:end_i], labels[i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess,run(cost, feed_dict={features:last_features, labels:last_labels})\n",
    "    valid_accuracy = sess.run(accuracy, feed_dict={features:valid_features, lavels:valid_labels})\n",
    "    print(\"Epoch: {:<4}- cost:{:<8.3} Valid accurqqcy:{:<5.3}\".format(epoch_i, current_cost, valid_accuracy))\n",
    "    \n",
    "n_input = 784 \n",
    "n_classes = 10\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
